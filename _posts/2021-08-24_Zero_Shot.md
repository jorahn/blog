---
toc: false
layout: post
description: Zero Shot Predictions with CLIP and Prompt Engineering
categories: [CLIP,Prompt Engineering]
title: Giving Zero Shot Predictions with CLIP a Try
---
The CLIP paper and repo show a straightforward way to use the model for zero shot image classification. 
CLIP consists of two model components: a vision model and a language model. While the language model
is a basic Transformer, the vision model can be either a modified ResNet or a VisionTransformer. 
The models have been pretrained on 400m image-text-pairs in a contrastive setting.


CLIP mainly provides APIs to encode images or text into this learned latent space.
After encoding a list of images, this can be utilized to find the image closest to an encoded text prompt.
Alternatively a list of text options can be encoded an utilized to find the best match to an encoded image.


The latter option is presented as Zero Shot Prediction, effectively creating a custom classifier from text 
options and then retrieving the classification results for an encoded image. It's called Zero Shot, because 
the CLIP model was never trained on these labels. 


As detailed by OpenAI in the CLIP paper an for example by the [https://twitter.com/BigscienceW/status/1429787756063043588?s=20]
(BigScience working group on Prompt Engineering), the framing of the classification label text playes a 
big role. E.g. instead of labels `cat` and `dog` a better performance can usually be achieved if the
labels `a photo of a cat` and `a photo of a dog` are chosen.


I've conducted some initial tests and found, that CLIP (ViT-B/16) can zero shot recognize patterns in fashion (e.g.
dots, stripes, flowers, checkered, print, ...) with ~80% accuracy without significant prompt engineering
or model fine tuning on additional data.


The work on [https://github.com/FreddeFrallan/Multilingual-CLIP](multilanguage) fine tuning of CLIP shows
interesting ways of more traditional optimization approaches building upon the extensively pretrained model.


More to come, no doubt!

---
toc: false
layout: post
description: Zero Shot Predictions with CLIP and Prompt Engineering
categories: [CLIP,Prompt Engineering]
title: Giving Zero Shot Predictions with CLIP a Try
---
The [CLIP paper](https://arxiv.org/abs/2103.00020) and repo show a straightforward way to use the model for zero shot image classification. 
CLIP consists of two model components: a vision model and a language model. While the language model
is a basic Transformer, the vision model can be either a modified ResNet or a VisionTransformer. 
The models have been pretrained on 400m image-text-pairs in a contrastive setting.


CLIP mainly provides APIs to encode images or text into this learned latent space.
After encoding a list of images, this can be utilized to find the image closest to an encoded text prompt.
Alternatively a list of text options can be encoded an utilized to find the best match to an encoded image.


The latter option is presented as Zero Shot Prediction, effectively creating a custom classifier from text 
options and then retrieving the classification results for an encoded image. It's called Zero Shot, because 
the CLIP model was never trained on these labels. 


As detailed by OpenAI in the CLIP paper and the [repo](https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb)
and is actively researched for example by the
[BigScience working group on Prompt Engineering](https://docs.google.com/presentation/d/1jj2MdxaJhmwckoN_d3_fQhyU0Qk5xaPALx2k4UYjA3Y/edit#slide=id.p) 
([also here](http://34.133.69.215:8501/)), 
the framing of the classification label text can play a big role in prediction performance. E.g. instead of 
labels `cat` vs. `dog` a better performance can usually be achieved if the labels `a photo of a cat` and 
`a photo of a dog` are chosen.


![image](https://user-images.githubusercontent.com/13120204/130806900-4e2623aa-7900-4be9-822b-2fcbd0e3847c.png "by Eric Jang https://blog.evjang.com/")


I've conducted some initial tests and found, that CLIP (ViT-B/16) can zero shot recognize patterns in fashion (e.g.
dots, stripes, flowers, checkered, print, ...) with ~80% accuracy without significant prompt engineering
or fine tuning on additional data.


The work on [multi-language](https://github.com/FreddeFrallan/Multilingual-CLIP) fine tuning of CLIP shows
additional ways of more traditional optimization approaches building upon the extensively pretrained CLIP model.
Last but not least, in combination with [Facebooks faiss](https://github.com/facebookresearch/faiss), I've
found CLIP to be quite useful as part of a high performance recommendation and/or search engine.


More to come, no doubt!

{
  
    
        "post0": {
            "title": "Giving Zero Shot Predictions with CLIP a Try",
            "content": "The CLIP paper and repo show a straightforward way to use the model for zero shot image classification. CLIP consists of two model components: a vision model and a language model. While the language model is a basic Transformer, the vision model can be either a modified ResNet or a VisionTransformer. The models have been pretrained on 400m image-text-pairs in a contrastive setting. . CLIP mainly provides APIs to encode images or text into this learned latent space. After encoding a list of images, this can be utilized to find the image closest to an encoded text prompt. Alternatively a list of text options can be encoded an utilized to find the best match to an encoded image. . The latter option is presented as Zero Shot Prediction, effectively creating a custom classifier from text options and then retrieving the classification results for an encoded image. It‚Äôs called Zero Shot, because the CLIP model was never trained on these labels. . As detailed by OpenAI in the CLIP paper and in more detail for example by the BigScience working group on Prompt Engineering, the framing of the classification label text playes a big role. E.g. instead of labels cat and dog a better performance can usually be achieved if the labels a photo of a cat and a photo of a dog are chosen. . I‚Äôve conducted some initial tests and found, that CLIP (ViT-B/16) can zero shot recognize patterns in fashion (e.g. dots, stripes, flowers, checkered, print, ‚Ä¶) with ~80% accuracy without significant prompt engineering or model fine tuning on additional data. . The work on multi-language fine tuning of CLIP shows interesting ways of more traditional optimization approaches building upon the extensively pretrained model. Last but not least, in combination with Facebooks faiss, I‚Äôve found CLIP to be quite useful as part of a high performance recommendation and/or search engine. . More to come, no doubt! .",
            "url": "https://jonathan.rahn.42digital.com/blog/clip/prompt%20engineering/2021/08/24/Zero_Shot.html",
            "relUrl": "/clip/prompt%20engineering/2021/08/24/Zero_Shot.html",
            "date": " ‚Ä¢ Aug 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Intro to My Blog",
            "content": "Why? . I‚Äôve wanted to put all my content into one place for some time. Putting things in writing requires more rigorous thinking, so this is mainly meant for myself. . Eventually, you may find my thoughts or ideas on topics like: . Machine Learning &amp; Data Science | Python Programming | Data Warehouses | Digital Advertising | and other topics‚Ä¶ | . Twitter . For now, let me leave you with a link to this wonderful piece by Ryan Moulton, that I recently discovered on Twitter: . Come, walk with me for a while through latent space.https://t.co/MbtvMyBB3t . &mdash; Ryan Moulton üíâüíâü•≥ (@moultano) July 21, 2021",
            "url": "https://jonathan.rahn.42digital.com/blog/about/2021/08/18/Intro.html",
            "relUrl": "/about/2021/08/18/Intro.html",
            "date": " ‚Ä¢ Aug 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Eternity Pool",
            "content": "Like so many others, I‚Äôve recently spent some time playing with VQGAN+CLIP. . Yesterday, this resulted in three new smartphone wallpapers. They are different steps in the VQGAN training process based on the prompt ‚Äúinfinity pool gazing into eternity with bright sunshine‚Äù: . 1 2 3 . | | | .",
            "url": "https://jonathan.rahn.42digital.com/blog/clip/art/2021/08/18/Eternity_pool.html",
            "relUrl": "/clip/art/2021/08/18/Eternity_pool.html",
            "date": " ‚Ä¢ Aug 18, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Tackling Imbalanced Regression with SMOGN",
            "content": "Imbalanced data is ubiquitous in practice and both research and high quality open source software exist for dealing with Imbalanced Classification tasks. Generally speaking, the approaches are under-sampling observations from classes that are dominating the data or over-sampling observations from rare classes. This can be done in naive ways (sampling with replacement) or in more sophisticated ways, e.g. by creating synthetic observations based on real observations with added noise. . When working on a Regression problem with imbalanced target variable distribution, I‚Äôve found three main approaches: . LogTransform of the Target Variable | Synthetic Minority Over-Sampling Technique for Regression - SMOGN | Deep Imbalanced Regression - DIR | LogTransform is in my standard toolbox and helped to boost performance as expected. But the imbalance/distribution skew was still far from resolved and thus predictions still centered in the range of most frequent observations. My Ridge, Gradient Boosting and FC-DNN models would completely ignore a significant range of (rare) target values in their predictions. Problem not solved! . Preprocessing the data with SMOGN took around 10 minuts and lead to a slightly worse MAPE performance, but a much better coverage of the entire target value range. SMOGN provides roughly three stages of customization for this re-sampling process. I‚Äôd characterize these as automated, automated within bounds and manual. So far I‚Äôve tested the automated preprocessing and while I‚Äôm satisfied with the result, the new distribution has become multimodal and still not ideal for training. But I‚Äôm optimistic that adding bounds or going for full manual configuration of the re-sampling should lead to further improvements. Problem still not solved, but a step in the right direction. . On my dataset, the total number of observations was slightly reduced after SMOGN upsampled rare observations and downsampled the frequent ones. Having taken care of all other preprocessing (missing data, numeric representations etc.), the number of columns was not affected. Lastly, I‚Äôve compared 2D principal component analysis of the original dataset and the same dataset after re-sampling with SMOGN and found the scatterplots to overlap very well. . So far, I‚Äôve not tried to apply DIR, as it seems a bit less straightforward, consisting of both label smoothing and feature smoothing and the examples include integration into the PyTorch model and training loop. So this might not be applicable to the Ridge and Gradient Boosting models I keep running for comparison. .",
            "url": "https://jonathan.rahn.42digital.com/blog/regression/imbalanced%20data/2018/08/24/Imbalanced-Regression.html",
            "relUrl": "/regression/imbalanced%20data/2018/08/24/Imbalanced-Regression.html",
            "date": " ‚Ä¢ Aug 24, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://jonathan.rahn.42digital.com/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://jonathan.rahn.42digital.com/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
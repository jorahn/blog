{
  
    
        "post0": {
            "title": "Fine-Tuning PyTorch Vision Models",
            "content": "After the surprising Image Classification results with zero-shot CLIP I‚Äôve run a test over a number of deep computer vision architectures from torchvision, fastai and the awesome timm library by Ross Wightman. . I‚Äôm using a 20-class subset of DeepFashion with around 100k images as the dataset. To accomodate memory requirements of different architectures, I‚Äôve set batch_size to 32 (this leaves room for improvements in smaller models). All models are pretrained on ImageNet, are built with a new initialized head and subsequently fine-tuned on DeepFashion for one epoch with frozen weights (new head only) and three epochs with unfrozen weights. . Training took on average ~20 minutes per epoch for all models on my hardware, so around 1:15h per model (the first epoch with frozen weights is generally a bit faster, than the others). The full run took around 18 hours in total. Eventually, I might do a few more runs, just to get a feel for the actual distribution of results. Here are the results: . Model Library Accuracy Training per epoch (mins) . resnet34 | torch | 0.428767 | 12:34 | . resnet50 | torch | 0.441164 | 18:08 | . resnet101 | torch | 0.446995 | 24:50 | . resnet152 | torch | 0.448151 | 32:15 | . xresnet34 | fastai | 0.326014 | 12:44 | . xresnext34 | fastai | 0.255726 | 15:24 | . efficientnet_b0 | timm | 0.382433 | 13:39 | . efficientnet_b2 | timm | 0.385690 | 16:47 | . efficientnet_b4 | timm | 0.360475 | 24:01 | . densenet121 | timm | 0.395409 | 20:14 | . inception_v4 | timm | 0.395409 | 23:00 | . inception_resnet_v2 | timm | 0.405652 | 25:18 | . mobilenetv3_large_100 | timm | 0.385007 | 11:40 | . vit_base_patch16_224 | timm | failed | n/a | . xception41 | timm | 0.389262 | 23:40 | . I was a little surprised, how well the good-old ResNets perform in this setting. Even the smaller ones. Certainly in terms of efficiency (cost/time to accuracy), they are a great choice. I guess, it‚Äôs partly due to the training setup (dataset, hardware, batch size, epochs ‚Ä¶), that some models didn‚Äôt perform better. But for my use case, this was quite informative. . I‚Äôm investigating, why the timm-ViT model failed to run my benchmark and also, how to run the same test with one of the pretrained CLIP vision models (ViT or ResNet). .",
            "url": "https://jorahn.github.io/blog/vision/fine%20tuning/2021/09/06/PyTorch-Timm-Vision.html",
            "relUrl": "/vision/fine%20tuning/2021/09/06/PyTorch-Timm-Vision.html",
            "date": " ‚Ä¢ Sep 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Embedded AI",
            "content": "With CLIPs impressive Image Classification capabilities, wouldn‚Äôt it be nice, to use them on embedded devices? But getting Pytorch and other dependencies to run on small ARM CPUs has to be a major challenge, or? And pretrained CLIP is certainly not a model made for embedded devices, like MobileNet. This can‚Äôt fit into memory, can it? . Lets dig out the old RaspberryPi 3 from some half forgotten box and order a camera module on Amazon for &lt; 10‚Ç¨. . . The capture is actually quite impressive for a cheap, embedded solution. Lighting might need some improvements, but resolution and focus should provide good material for detection. . . Fast forward pytorch &amp; clip setup steps... . Let‚Äôs try to get some CLIP demo code running on the Pi: . . And now zero shot predict the (resized) camera output: . . Next up: image classification on real time stream from the camera. Let‚Äôs see some latency measurements for clip. . Not actually expected this to work... #openai #clip (RN101) inference on #raspberrypi 3. What if we&#39;d connect the camera module thoughü§î #embeddedclip pic.twitter.com/GniFbQvOll . &mdash; Jonathan Rahn (@rahnjonathan) August 26, 2021",
            "url": "https://jorahn.github.io/blog/clip/embedded/raspberrypi/2021/08/26/Embedded_AI.html",
            "relUrl": "/clip/embedded/raspberrypi/2021/08/26/Embedded_AI.html",
            "date": " ‚Ä¢ Aug 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Tackling Imbalanced Regression with SMOGN",
            "content": "Imbalanced data is ubiquitous in practice and both research and high quality open source software exist for dealing with Imbalanced Classification tasks. Generally speaking, the main approaches are under-sampling observations from classes that are observed frequently or over-sampling observations from rare classes to achieve a balanced distribution of labels. This can be done in naive ways (sampling with replacement) or in more sophisticated ways, e.g. by creating synthetic observations based on real observations with interpolations or added noise. . When working on a Regression problem with highly skewed target variable distribution, I‚Äôve found it much more difficult to come up with useful approaches vs. Classification. After some research, I have settled on three options to test individually and in combination: . LogTransform of the Target Variable | Synthetic Minority Over-Sampling Technique for Regression - SMOGN | Deep Imbalanced Regression - DIR | LogTransform is in my standard toolbox and helped to boost performance as expected. But the imbalance/distribution was still skewed and far from normally distributed and thus predictions still centered in the range of most frequent observations. My Ridge, Gradient Boosting and FC-DNN models would completely ignore a significant range of (rare) target values in their predictions. Problem not solved! . Preprocessing the data with SMOGN took around 10 minuts and (applied in combination with LogTransform) lead to a slightly worse MAPE performance, but a much better coverage of the entire target value range. SMOGN provides roughly three stages of customization for the re-sampling process. I‚Äôd characterize these as automated, automated within bounds and manual. So far I‚Äôve tested the automated preprocessing and while I‚Äôm satisfied with the result, the new distribution is multimodal and still not ideal for training. But I‚Äôm optimistic that adding bounds or going for fully manual configuration of the re-sampling should lead to further improvements. Problem still not solved, but a step in the right direction. . Before After . | | . On my dataset, the total number of observations was reduced by ~20% after SMOGN upsampled rare observations with noise and downsampled the frequent ones. Having taken care of all other preprocessing (missing data, numeric representations etc.), the number of columns was not affected. Missing values would have resulted in dropped columns. Lastly, I‚Äôve compared a 2D PCA-plot of the original preprocessed dataset and the same dataset after additional re-sampling with SMOGN and found the scatterplots to overlap very well. . . So far, I‚Äôve not tried to apply DIR, as it seems a bit less straightforward, consisting of label smoothing and feature smoothing as separate steps. Also the examples include integration into the PyTorch model and training loop. So this might not be applicable to the Ridge and Gradient Boosting models I keep running for comparison, but a learning resampling instead of static one-off preprocessing, could yield even better results. .",
            "url": "https://jorahn.github.io/blog/regression/imbalanced%20data/2021/08/24/Imbalanced_Regression.html",
            "relUrl": "/regression/imbalanced%20data/2021/08/24/Imbalanced_Regression.html",
            "date": " ‚Ä¢ Aug 24, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Giving Zero Shot Predictions with CLIP a Try",
            "content": "The CLIP paper and repo show a straightforward way to use the model for zero shot image classification. CLIP consists of two model components: a vision model and a language model. While the language model is a basic Transformer, the vision model can be either a modified ResNet or a VisionTransformer. The models have been pretrained on 400m image-text-pairs in a contrastive setting. . CLIP mainly provides APIs to encode images or text into this learned latent space. After encoding a list of images, this can be utilized to find the image closest to an encoded text prompt. Alternatively a list of text options can be encoded an utilized to find the best match to an encoded image. . The latter option is presented as Zero Shot Prediction, effectively creating a custom classifier from text options and then retrieving the classification results for an encoded image. It‚Äôs called Zero Shot, because the CLIP model was never trained on these labels. . As detailed by OpenAI in the CLIP paper and the repo and is actively researched for example by the BigScience working group on Prompt Engineering (also here), the framing of the classification label text can play a big role in prediction performance. E.g. instead of labels cat vs. dog a better performance can usually be achieved if the labels a photo of a cat and a photo of a dog are chosen. . . I‚Äôve conducted some initial tests and found, that CLIP (ViT-B/16) can zero shot recognize patterns in fashion (e.g. dots, stripes, flowers, checkered, print, ‚Ä¶) with ~60% accuracy without significant prompt engineering or fine tuning on additional data. . The work on multi-language fine tuning of CLIP shows additional ways of more traditional optimization approaches building upon the extensively pretrained CLIP model. Last but not least, in combination with Facebooks faiss, I‚Äôve found CLIP to be quite useful as part of a high performance recommendation and/or search engine. . More to come, no doubt! .",
            "url": "https://jorahn.github.io/blog/clip/prompt%20engineering/2021/08/23/Zero_Shot.html",
            "relUrl": "/clip/prompt%20engineering/2021/08/23/Zero_Shot.html",
            "date": " ‚Ä¢ Aug 23, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Eternity Pool",
            "content": "Like so many others, I‚Äôve recently spent some time playing with VQGAN+CLIP. . Yesterday, this resulted in three new smartphone wallpapers. They are different steps in the VQGAN training process based on the prompt ‚Äúinfinity pool gazing into eternity with bright sunshine‚Äù: . 1 2 3 . | | | .",
            "url": "https://jorahn.github.io/blog/clip/art/2021/08/19/Eternity_pool.html",
            "relUrl": "/clip/art/2021/08/19/Eternity_pool.html",
            "date": " ‚Ä¢ Aug 19, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Intro to My Blog",
            "content": "Why? . I‚Äôve wanted to put all my content into one place for some time. Putting things in writing requires more rigorous thinking, so this is mainly meant for myself. . Eventually, you may find my thoughts or ideas on topics like: . Machine Learning &amp; Data Science | Python Programming | Data Warehouses | Digital Advertising | and other topics‚Ä¶ | . Twitter . For now, let me leave you with a link to this wonderful piece by Ryan Moulton, that I recently discovered on Twitter: . Come, walk with me for a while through latent space.https://t.co/MbtvMyBB3t . &mdash; Ryan Moulton üíâüíâü•≥ (@moultano) July 21, 2021",
            "url": "https://jorahn.github.io/blog/about/2021/08/18/Intro.html",
            "relUrl": "/about/2021/08/18/Intro.html",
            "date": " ‚Ä¢ Aug 18, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". See: Intro to My Blog . This website is powered by fastpages. . Impressum: . Verantwortlich f√ºr den Inhalt nach ¬ß 55 Abs. 2 RStV . RCS Analytics GmbH Bruno-Lauenroth-Weg 31 22417 Hamburg .",
          "url": "https://jorahn.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://jorahn.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}